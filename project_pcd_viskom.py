# -*- coding: utf-8 -*-
"""Project PCD-VISKOM .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-fWbBWdHCpKGC928MR-s1CUwHljlX4AE

# Deteksi Emosi Karyawan Berbasis Wajah Menggunakan DenseNet-169

**Mata Kuliah**: Pengolahan Citra Digital dan Visi Komputer  
**Anggota** : Muhammad Makarim (225150207111122), Rakha Alif Athallah (2225150207111050)

## Tujuan
Mendeteksi emosi karyawan dari citra wajah menggunakan metode kombinasi:
1. Konversi Ruang Warna
2. Peningkatan Kualitas Citra (Histogram Equalization / CLAHE)
3. Pengenalan Wajah (Haarcascade) + Klasifikasi Emosi dengan CNN (DenseNet-169, transfer learning)

Output utama:
- Model klasifikasi emosi berbasis DenseNet-169
- Evaluasi performa (akurasi, F1-score, confusion matrix)
- Visualisasi Grad-CAM untuk interpretasi fitur wajah

# **Import Library dan Dataset**
"""

!pip install timm torchmetrics grad-cam gdown

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
from PIL import Image
from glob import glob
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T

import timm
import torchmetrics
from sklearn.metrics import confusion_matrix, classification_report

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

IM_SIZE = 224
BATCH_SIZE = 16
LR = 1e-4
EPOCHS = 20

GDRIVE_DATASET_URL = "https://drive.google.com/drive/folders/1cOHBFFUHI_acdK2KTMgsxz2sTQ8n48Ai?usp=drive_link"

import gdown
RAW_DATA_DIR = "/content/dataset_raw"
EXTRACT_DIR = "/content/dataset_faces"
os.makedirs(RAW_DATA_DIR, exist_ok=True)
os.makedirs(EXTRACT_DIR, exist_ok=True)

print("Mengunduh dataset dari Google Drive...")
gdown.download_folder(GDRIVE_DATASET_URL, output=RAW_DATA_DIR, quiet=False, use_cookies=False)

zip_files = glob(os.path.join(RAW_DATA_DIR, "*.zip"))
if len(zip_files) == 0:
    raise FileNotFoundError(f"Tidak ditemukan file .zip di {RAW_DATA_DIR}. Pastikan folder GDrive berisi dataset .zip.")

zip_path = zip_files[0]
print("Zip dataset ditemukan:", zip_path)

print("Mengekstrak zip...")
!unzip -q "{zip_path}" -d "{EXTRACT_DIR}"
print("Selesai ekstrak.")

DATASET_ROOT = EXTRACT_DIR

print("DATASET_ROOT:", DATASET_ROOT)
print("Isi DATASET_ROOT:")
!ls "{DATASET_ROOT}"

"""# **Eksplorasi Dataset & Struktur Folder**"""

train_dir = os.path.join(DATASET_ROOT, "train")
class_names = sorted(os.listdir(train_dir))
print("Classes:", class_names)

class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}
idx_to_class = {v: k for k, v in class_to_idx.items()}
print("Mapping:", class_to_idx)

def count_images_per_class(split):
    split_dir = os.path.join(DATASET_ROOT, split)
    counts = {}
    for cls in class_names:
        folder = os.path.join(split_dir, cls)
        counts[cls] = len(glob(os.path.join(folder, "*.*")))
    return counts

for split in ["train", "val", "test"]:
    print(f"=== {split.upper()} ===")
    print(count_images_per_class(split))

example_paths = glob(os.path.join(train_dir, class_names[0], "*.*"))[:5]

plt.figure(figsize=(10, 4))
for i, p in enumerate(example_paths):
    img = cv2.imread(p)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.subplot(1, len(example_paths), i+1)
    plt.imshow(img_rgb)
    plt.axis("off")
plt.suptitle(f"Contoh citra mentah - kelas: {class_names[0]}")
plt.show()

"""# **Pre-processing & Face Detection**

## **Load Haarcascade dan definisikan fungsi CLAHE**
"""

haar_path = cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
face_cascade = cv2.CascadeClassifier(haar_path)

def preprocess_and_detect_face(bgr_image, return_bbox=False):
    """
    Input:
        bgr_image: numpy array (BGR, dari cv2.imread)
    Output:
        face_gray_eq: citra wajah hasil CLAHE (grayscale)
        (opsional) bbox: (x, y, w, h)
    """
    gray = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)

    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    gray_eq = clahe.apply(gray)

    faces = face_cascade.detectMultiScale(gray_eq, scaleFactor=1.3, minNeighbors=5)

    if len(faces) == 0:
            face_roi = gray_eq
        bbox = None
    else:
          x, y, w, h = sorted(faces, key=lambda f: f[2]*f[3], reverse=True)[0]
        face_roi = gray_eq[y:y+h, x:x+w]
        bbox = (x, y, w, h)

    if return_bbox:
        return face_roi, bbox
    else:
        return face_roi

"""## **Visualisasi sebelum & sesudah preprocessing**"""

sample_path = example_paths[0]
bgr = cv2.imread(sample_path)
face_roi, bbox = preprocess_and_detect_face(bgr, return_bbox=True)

plt.figure(figsize=(10, 4))

plt.subplot(1, 3, 1)
plt.title("Citra Mentah (RGB)")
plt.imshow(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))
plt.axis("off")

plt.subplot(1, 3, 2)
plt.title("Grayscale + CLAHE")
plt.imshow(face_roi, cmap="gray")
plt.axis("off")

if bbox is not None:
    x, y, w, h = bbox
    bgr_box = bgr.copy()
    cv2.rectangle(bgr_box, (x,y), (x+w, y+h), (0,255,0), 2)
    plt.subplot(1, 3, 3)
    plt.title("Deteksi Wajah (Haarcascade)")
    plt.imshow(cv2.cvtColor(bgr_box, cv2.COLOR_BGR2RGB))
    plt.axis("off")

plt.tight_layout()
plt.show()

"""# **PyTorch Transform, Dataset, dan DataLoader**

## **Transform PyTorch (Resize + Grayscaleâ†’3ch + Normalize)**
"""

mean = [0.485, 0.456, 0.406]
std  = [0.229, 0.224, 0.225]

transform = T.Compose([
    T.Resize((IM_SIZE, IM_SIZE)),
    T.Grayscale(num_output_channels=3),
    T.ToTensor(),
    T.Normalize(mean=mean, std=std)
])

"""# **Custom Dataset**"""

class EmotionFaceDataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        self.root_dir = os.path.join(root_dir, split)
        self.transform = transform

        self.image_paths = []
        self.labels = []

        for cls_name in class_names:
            cls_dir = os.path.join(self.root_dir, cls_name)
            for p in glob(os.path.join(cls_dir, "*.*")):
                self.image_paths.append(p)
                self.labels.append(class_to_idx[cls_name])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        bgr = cv2.imread(img_path)

        face_gray_eq = preprocess_and_detect_face(bgr)

        pil_img = Image.fromarray(face_gray_eq)

        if self.transform is not None:
            img_tensor = self.transform(pil_img)
        else:
            img_tensor = T.ToTensor()(pil_img)

        return img_tensor, label

train_ds = EmotionFaceDataset(DATASET_ROOT, split="train", transform=transform)
val_ds   = EmotionFaceDataset(DATASET_ROOT, split="val",   transform=transform)
test_ds  = EmotionFaceDataset(DATASET_ROOT, split="test",  transform=transform)

train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
test_dl  = DataLoader(test_ds,  batch_size=1, shuffle=False, num_workers=2)

len(train_ds), len(val_ds), len(test_ds)

"""# **Visualisasi Data Setelah Pre-processing**"""

def show_batch(dl, n_images=8):
    imgs, labels = next(iter(dl))
    plt.figure(figsize=(12, 4))
    for i in range(min(n_images, len(imgs))):
        img = imgs[i]
        img_np = img.numpy()
        img_np = np.transpose(img_np, (1, 2, 0))
        img_np = img_np * np.array(std) + np.array(mean)
        img_np = np.clip(img_np, 0, 1)

        plt.subplot(2, n_images//2, i+1)
        plt.imshow(img_np)
        plt.title(idx_to_class[labels[i].item()])
        plt.axis("off")
    plt.tight_layout()
    plt.show()

show_batch(train_dl)

"""# **Definisi Model DenseNet (Transfer Learning)**"""

NUM_CLASSES = len(class_names)

def create_densenet_model(num_classes, pretrained=True, freeze_feature_extractor=False):
    model = timm.create_model(
        "densenet169",
        pretrained=pretrained,
        num_classes=num_classes
    )

    if freeze_feature_extractor:
        for name, param in model.named_parameters():
            param.requires_grad = False

        for param in model.get_classifier().parameters():
            param.requires_grad = True

    return model.to(device)

model = create_densenet_model(NUM_CLASSES, pretrained=True, freeze_feature_extractor=False)
print(model)

"""# **Training Utility (loss, optimizer, metric, loop)**

## **Setup loss, optimizer, metrics, scheduler**
"""

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

f1_metric = torchmetrics.F1Score(
    task="multiclass",
    num_classes=NUM_CLASSES,
    average="macro"
).to(device)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode="min",
    patience=2,
    factor=0.5
)

def get_lr(optimizer):
    return optimizer.param_groups[0]["lr"]

"""## **Fungsi train_epoch & eval_epoch**"""

def train_one_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    running_correct = 0
    total = 0
    f1_metric.reset()

    for imgs, labels in dataloader:
        imgs = imgs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * imgs.size(0)
        _, preds = torch.max(outputs, 1)
        running_correct += (preds == labels).sum().item()
        total += labels.size(0)

        f1_metric.update(preds, labels)

    epoch_loss = running_loss / total
    epoch_acc = running_correct / total
    epoch_f1  = f1_metric.compute().item()

    return epoch_loss, epoch_acc, epoch_f1

def eval_one_epoch(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    running_correct = 0
    total = 0
    f1_metric.reset()

    with torch.no_grad():
        for imgs, labels in dataloader:
            imgs = imgs.to(device)
            labels = labels.to(device)

            outputs = model(imgs)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * imgs.size(0)
            _, preds = torch.max(outputs, 1)
            running_correct += (preds == labels).sum().item()
            total += labels.size(0)

            f1_metric.update(preds, labels)

    epoch_loss = running_loss / total
    epoch_acc = running_correct / total
    epoch_f1  = f1_metric.compute().item()

    return epoch_loss, epoch_acc, epoch_f1

"""## **Main training loop + save best model (ke Drive)**"""

best_val_f1 = 0.0
history = {
    "train_loss": [],
    "val_loss": [],
    "train_acc": [],
    "val_acc": [],
    "train_f1": [],
    "val_f1": []
}

SAVE_DIR = "/content/drive/MyDrive/PROJECT_EMOSI/models"
os.makedirs(SAVE_DIR, exist_ok=True)
BEST_MODEL_PATH = os.path.join(SAVE_DIR, "densenet169_emosi_best.pth")

for epoch in range(EPOCHS):
    print(f"Epoch [{epoch+1}/{EPOCHS}]")

    train_loss, train_acc, train_f1 = train_one_epoch(model, train_dl, optimizer, criterion, device)
    val_loss, val_acc, val_f1       = eval_one_epoch(model, val_dl, criterion, device)

    scheduler.step(val_loss)
    print(f"Learning rate saat ini: {get_lr(optimizer):.6f}")

    history["train_loss"].append(train_loss)
    history["val_loss"].append(val_loss)
    history["train_acc"].append(train_acc)
    history["val_acc"].append(val_acc)
    history["train_f1"].append(train_f1)
    history["val_f1"].append(val_f1)

    print(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")
    print(f"Train Acc:  {train_acc:.4f} | Val Acc:  {val_acc:.4f}")
    print(f"Train F1:   {train_f1:.4f} | Val F1:   {val_f1:.4f}")

    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save(model.state_dict(), BEST_MODEL_PATH)
        print(">> Best model saved!")

"""# **Plot Learning Curves (Loss, Accuracy, F1)**"""

epochs_range = range(1, len(history["train_loss"]) + 1)

plt.figure(figsize=(14,4))
# Loss
plt.subplot(1,3,1)
plt.plot(epochs_range, history["train_loss"], label="Train Loss")
plt.plot(epochs_range, history["val_loss"], label="Val Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)

# Accuracy
plt.subplot(1,3,2)
plt.plot(epochs_range, history["train_acc"], label="Train Acc")
plt.plot(epochs_range, history["val_acc"], label="Val Acc")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)

# F1-score
plt.subplot(1,3,3)
plt.plot(epochs_range, history["train_f1"], label="Train F1")
plt.plot(epochs_range, history["val_f1"], label="Val F1")
plt.xlabel("Epoch")
plt.ylabel("F1-score")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""# **Evaluasi Akhir pada Test Set**"""

# Load best model
best_model = create_densenet_model(NUM_CLASSES, pretrained=False, freeze_feature_extractor=False)
best_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))
best_model.to(device)
best_model.eval()

all_labels = []
all_preds  = []

with torch.no_grad():
    for imgs, labels in test_dl:
        imgs = imgs.to(device)
        labels = labels.to(device)

        outputs = best_model(imgs)
        _, preds = torch.max(outputs, 1)

        all_labels.extend(labels.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())

# Confusion matrix & classification report
cm = confusion_matrix(all_labels, all_preds)
print("Classification Report:\n")
print(classification_report(all_labels, all_preds, target_names=class_names))

import seaborn as sns

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Prediksi")
plt.ylabel("Ground Truth")
plt.title("Confusion Matrix - DenseNet-169 Emosi")
plt.show()

"""# **Grad-CAM untuk Visualisasi Area Wajah Penting**"""

from pytorch_grad_cam import GradCAMPlusPlus
from pytorch_grad_cam.utils.image import show_cam_on_image

def tensor_to_image_np(tensor):
    img = tensor.cpu().numpy()
    img = np.transpose(img, (1, 2, 0))  # C,H,W -> H,W,C
    img = img * np.array(std) + np.array(mean)
    img = np.clip(img, 0, 1)
    return img

best_model.eval()

target_layers = [best_model.features[-1]]

test_iter = iter(test_dl)
imgs, labels = next(test_iter)
imgs = imgs.to(device)
labels = labels.to(device)

with GradCAMPlusPlus(model=best_model, target_layers=target_layers) as cam:
    grayscale_cams = cam(input_tensor=imgs)

plt.figure(figsize=(10, 6))
for i in range(min(4, len(imgs))):
    img_np = tensor_to_image_np(imgs[i])
    grayscale_cam = grayscale_cams[i]
    visualization = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)

    plt.subplot(2, 4, i+1)
    plt.imshow(img_np)
    plt.title(f"GT: {idx_to_class[labels[i].item()]}")
    plt.axis("off")

    plt.subplot(2, 4, i+1+4)
    plt.imshow(visualization)
    plt.title("Grad-CAM")
    plt.axis("off")

plt.tight_layout()
plt.show()

"""# **Demo Inference**"""

def predict_single_image(img_path):
    bgr = cv2.imread(img_path)
    face_roi = preprocess_and_detect_face(bgr)
    pil_img = Image.fromarray(face_roi)
    img_tensor = transform(pil_img).unsqueeze(0).to(device)

    best_model.eval()
    with torch.no_grad():
        outputs = best_model(img_tensor)
        probs = torch.softmax(outputs, dim=1)
        pred_idx = torch.argmax(probs, dim=1).item()
        pred_cls = idx_to_class[pred_idx]

    return pred_cls, probs.cpu().numpy()[0]

test_image = example_paths[1]
pred_cls, probs = predict_single_image(test_image)
print("Predicted:", pred_cls)
print("Probabilities:", probs)

"""# **Save Model**"""

BEST_MODEL_PATH = "/content/drive/MyDrive/PROJECT_EMOSI/models/densenet169_emosi_best.pth"
torch.save(model.state_dict(), BEST_MODEL_PATH)

requirements_text = """
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0
timm>=0.9.0
torchmetrics>=1.2.0
pytorch-grad-cam>=1.4.6
opencv-python
Pillow>=10.0.0
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
streamlit>=1.30.0
streamlit-webrtc>=0.47.0
plotly>=5.18.0
av>=12.0.0
gdown>=5.0.0
""".strip()

with open("requirements.txt", "w") as f:
    f.write(requirements_text)

print("requirements.txt berhasil dibuat di folder kerja saat ini.")

"""# **Ringkasan**

## Ringkasan

Pada proyek ini, kami membangun sistem deteksi emosi karyawan dari citra wajah dengan pipeline:

1. **Pengenalan Wajah (Haarcascade)** - mendeteksi dan memotong wajah dari citra.
2. **Konversi Ruang Warna + CLAHE** - meningkatkan kualitas citra wajah melalui konversi ke grayscale dan peningkatan kontras lokal.
3. **Klasifikasi Emosi berbasis CNN (DenseNet-169, transfer learning)** - memetakan citra wajah menjadi label emosi.

Model dievaluasi menggunakan akurasi, F1-score, dan confusion matrix.  
Grad-CAM digunakan untuk memvisualisasikan area wajah yang paling berkontribusi terhadap prediksi emosi.
"""